# -*- coding: utf-8 -*-
"""CoffeDeepLearningAnalyst.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hEW6vGJKk4LKJFqb1URc-eabin44HVcI
"""

from google.colab import drive
drive.mount('/content/gdrive')
import os
os.chdir('/content/gdrive/My Drive/veri')

#create train, test, and validation sets

# Gerekli kütüphaneleri içe aktar
import os
import shutil
import random

# Rastgele sayı üretebilmek için seed belirle (tekrarlanabilirlik için)
random.seed(123)

# Veri setinin bulunduğu dizin yolu
data_path = '/content/gdrive/MyDrive/veri/256x'

# Her sınıfın dizinini oluşturmak için yolları birleştir
datas = [os.path.join(data_path, d) for d in os.listdir(data_path)]

# Eğitim, test ve doğrulama dizinlerini oluştur
train_path = './train'
test_path = './test'

# Eğitim ve test dizinlerini oluştur
os.mkdir(train_path)
os.mkdir(test_path)

# Her sınıf için eğitim ve test dizinlerini oluştur
for d in datas:
    os.mkdir(os.path.join(train_path, os.path.basename(d)))
    os.mkdir(os.path.join(test_path, os.path.basename(d)))

# Veriyi eğitim, test ve doğrulama setlerine bölelim
for d in datas:
    # Eğer dizin ".DS_Store" ise devam et (macOS'tan gelen gizli dosya)
    if d == './data/.DS_Store':
        continue

    # Dizin içindeki dosyaları al
    files = os.listdir(d)
    # Dosyaları karıştır
    random.shuffle(files)

    # Eğitim ve test setlerini oluştur
    train_files = files[:int(len(files) * 0.8)]
    test_files = files[int(len(files) * 0.8):]

    # Eğitim setini oluşturulan dizine kopyala
    for f in train_files:
        shutil.copy(os.path.join(d, f), os.path.join(train_path, os.path.basename(d)))

    # Test setini oluşturulan dizine kopyala
    for f in test_files:
        shutil.copy(os.path.join(d, f), os.path.join(test_path, os.path.basename(d)))

# İşlem tamamlandığında mesaj yazdır
print('Done!')

import tensorflow as tf
from keras.preprocessing.image import ImageDataGenerator

# Eğitim veri artırma ve ölçeklendirme işlemleri için ImageDataGenerator oluştur
train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # %20'si doğrulama için ayrılacak

# Eğitim seti için veri akışını oluştur
train_generator = train_datagen.flow_from_directory(
    'train/',                  # Eğitim veri setinin bulunduğu dizin
    target_size=(150, 150),     # Tüm görüntülerin boyutunu (yükseklik, genişlik) ayarla
    batch_size=32,              # Mini grup boyutu
    class_mode='binary',        # Sınıflandırma türü (binary: ikili sınıflandırma)
    subset='training')          # Veri setinin alt kümesi (eğitim için)

# Doğrulama seti için veri akışını oluştur
validation_generator = train_datagen.flow_from_directory(
    'train/',                  # Eğitim veri setinin bulunduğu dizin
    target_size=(150, 150),     # Tüm görüntülerin boyutunu (yükseklik, genişlik) ayarla
    batch_size=32,              # Mini grup boyutu
    class_mode='binary',        # Sınıflandırma türü (binary: ikili sınıflandırma)
    subset='validation')        # Veri setinin alt kümesi (doğrulama için)

# Test seti için veri artırma ve ölçeklendirme işlemleri için ImageDataGenerator oluştur
test_datagen = ImageDataGenerator(rescale=1./255)

# Test seti için veri akışını oluştur
test_generator = test_datagen.flow_from_directory(
    'test/',                   # Test veri setinin bulunduğu dizin
    target_size=(150, 150),     # Tüm görüntülerin boyutunu (yükseklik, genişlik) ayarla
    batch_size=32,              # Mini grup boyutu
    class_mode='binary')        # Sınıflandırma türü (binary: ikili sınıflandırma)

import tensorflow as tf

# Sequential model oluştur
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    # Daha fazla evrişim ve havuzlama katmanları eklenebilir
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Modeli derle
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# Modeli eğit
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,  # Her epoch'ta kaç adım yapılacağı
    epochs=30,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // validation_generator.batch_size)  # Doğrulama için adım sayısı

# Eğitim ve doğrulama kayıplarını ve doğruluklarını görselleştir
import matplotlib.pyplot as plt

# Eğitim ve validation loss
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Eğitim ve validation accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

#6 - Validasyon verileri birleştirildi
all_train_datagen = ImageDataGenerator(rescale=1./255)

# Tüm eğitim veri seti için veri akışını oluştur
all_train_generator = all_train_datagen.flow_from_directory(
    'train/',                  # Eğitim veri setinin bulunduğu dizin
    target_size=(150, 150),     # Tüm görüntülerin boyutunu (yükseklik, genişlik) ayarla
    batch_size=32,              # Mini grup boyutu
    class_mode='binary')        # Sınıflandırma türü (binary: ikili sınıflandırma)

# Modeli oluştur
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    # Daha fazla katman eklenebilir
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Modeli derle
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# Modeli tüm eğitim veri seti üzerinde eğit
history = model.fit(
    all_train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,  # Her epoch'ta kaç adım yapılacağı
    epochs=8)

#7. test verileri ile modelin performansı
# Modeli test veri seti üzerinde değerlendir
test_loss, test_acc = model.evaluate(test_generator, verbose=2)
print('\nTest accuracy:', test_acc)

# 8. add dropout
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    # Daha fazla katman eklenebilir
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),  # Dropout katmanı eklendi
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Modeli derle
model.compile(loss='binary_crossentropy',
                optimizer='adam',
                metrics=['accuracy'])

# Modeli eğit
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,  # Her epoch'ta kaç adım yapılacağı
    epochs=30,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // validation_generator.batch_size)  # Doğrulama için adım sayısı

# Eğitim ve doğrulama kayıplarını ve doğruluklarını görselleştir
import matplotlib.pyplot as plt

# Eğitim ve validation loss
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Eğitim ve validation accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

import tensorflow as tf
from keras.preprocessing.image import ImageDataGenerator
from keras.applications import VGG16
from keras import models
from keras import layers

# VGG16 modelini ImageNet veri kümesinde eğitilmiş ağırlıklarıyla yükle
vgg16_base = VGG16(weights='imagenet',
                   include_top=False,
                   input_shape=(150, 150, 3))  # Giriş boyutunu ihtiyaca göre ayarla

# Evrişim tabanını dondur
vgg16_base.trainable = False

# Kendi sınıflandırıcınızı VGG16 tabanının üstüne oluştur
model = models.Sequential()
model.add(vgg16_base)
model.add(layers.Flatten())
model.add(layers.Dense(256, activation='relu'))  # İhtiyaca göre ayarla
model.add(layers.Dropout(0.5))  # Gerekirse düzenleme için dropout ekleyin
model.add(layers.Dense(1, activation='sigmoid'))  # İkili sınıflandırma varsayıldığı için

# Modeli derle
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# Modelin değişikliklerini kontrol etmek için bir özetini yazdır
model.summary()


# Modeli eğit
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    epochs=5,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // validation_generator.batch_size)

import matplotlib.pyplot as plt

# Eğitim ve doğrulama loss
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Eğitim Kaybı')
plt.plot(history.history['val_loss'], label='Doğrulama Kaybı')
plt.title('Eğitim ve Doğrulama Kaybı')
plt.xlabel('Epoch')
plt.ylabel('Kayıp')
plt.legend()

# Eğitim ve doğrulama doğruluk
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Eğitim Doğruluğu')
plt.plot(history.history['val_accuracy'], label='Doğrulama Doğruluğu')
plt.title('Eğitim ve Doğrulama Doğruluğu')
plt.xlabel('Epoch')
plt.ylabel('Doğruluk')
plt.legend()

plt.tight_layout()
plt.show()

# 2. Madde

# Veri seti, eğitim ve test setlerine ayrılmıştır; %80'i eğitim ve %20'si test için ayrılmıştır. Bu yaygın bir bölme yöntemidir ve eğitim sırasında modelin performansını izlemek için bir doğrulama setinin (eğitim setinin bir alt kümesi) kullanılması iyidir.
# Veri Artırma:
# Veri artırma, model genelleştirmesini iyileştirmek için eğitim setine uygulanır. Ancak unutulmamalıdır ki artırma parametreleri (örneğin, döndürme, yakınlaştırma, çevirme) veri setinizin özelliklerine bağlı olarak ayarlanması gerekebilir.
# Model Mimarisi:
# Başlangıç modeli, tek bir evrişim katmanından sonra gelen maksimum havuzlama, düzleştirme katmanı, 64 nöronlu ReLU aktivasyonlu yoğun bir katman ve ikili sınıflandırma için bir nöron ve sigmoid aktivasyonlu son yoğun bir katman ile basit bir mimariye sahiptir.
# Eğitim Parametreleri:
# Model, ikili çapraz entropi kaybı ve Adam optimizasyonu kullanılarak derlenir. Eğitim sırasında izlenecek metrikler doğruluktur.
# Eğitim ve Değerlendirme:
# Model, 30 epoch boyunca eğitilir ve eğitim ile doğrulama doğrulukları ve kayıpları çizilir. Daha sonra model, ek 8 epoch boyunca tüm eğitim verileri üzerinde eğitilir. Son olarak, model test setinde değerlendirilir ve test doğruluğu yazdırılır.
# Model Modifikasyonu - Dropout:
# Modelde aşırı uyumun önlenmesi için dropout oranı 0.5 olan bir dropout katmanı eklenir. Model daha sonra tekrar derlenir, eğitilir ve eğitim ile doğrulama doğrulukları ve kayıpları tekrar çizilir.
# Transfer Öğrenme (VGG16):
# Transfer öğrenme, ImageNet üzerinde önceden eğitilmiş ağırlıklarla VGG16 tabanını kullanarak uygulanır. Evrişim tabanı dondurulur ve üzerine özel bir sınıflandırıcı eklenir. Model derlenir ve eğitilir.